{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/StatQuest-Neural-Networks-and-AI/blob/main/chapter_11/chapter_11_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "484f0f25-6c2b-4753-85c3-ec1c6c6b704a",
      "metadata": {
        "id": "484f0f25-6c2b-4753-85c3-ec1c6c6b704a"
      },
      "source": [
        "# The StatQuest Illustrated Guide to Neural Networks and AI\n",
        "## Chapter 11 - Attention\n",
        "\n",
        "Copyright 2024, Joshua Starmer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5f73041-bc4a-48f9-9d14-e516a4d21397",
      "metadata": {
        "id": "f5f73041-bc4a-48f9-9d14-e516a4d21397"
      },
      "source": [
        "-----\n",
        "\n",
        "This tutorial is from the book, **[The StatQuest Illustrated Guide to Neural Networks and AI](https://www.amazon.com/dp/B0DRS71QVQ)**.\n",
        "\n",
        "In this notebook, we will build and train a Seq2Seq/Encoder-Deocder model Attention. Compared with what we did in Chapter 10, the Encoder and Decoder in this example will be a little simplier, although it doesn't technically need to be - we can make it as fancy as we want.  Anyway, this model only uses a single layer of LSTMs and no stacking, as seen in the picture below.\n",
        "\n",
        "<img src=\"https://github.com/StatQuest/signa/blob/main/chapter_11/images/full_model.png?raw=1\" alt=\"an encoder-decoder model with attention\" style=\"width: 800px;\">\n",
        "\n",
        "In this tutorial, you will...\n",
        "\n",
        "**NOTE:**\n",
        "\n",
        "This tutorial assumes that you have read through the chapter on **Seq2Seq and Encoder-Decoder Models** and the chapter on **Attention** in **The StatQuest Illustrated Guide to Neural Networks and AI**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d3e3b1e-ea6d-4efb-be2f-a52abc6c8a08",
      "metadata": {
        "id": "1d3e3b1e-ea6d-4efb-be2f-a52abc6c8a08"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d995f41b-1786-48a6-a849-d17a08c5e6e0",
      "metadata": {
        "id": "d995f41b-1786-48a6-a849-d17a08c5e6e0"
      },
      "source": [
        "# Import the modules that will do all the work\n",
        "\n",
        "The very first thing we need to do is load a bunch of Python modules. Python itself is just a basic programming language. These modules give us extra functionality to create and train a Neural Network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d771ba19-0a4e-47e9-be6f-b8b6bd3a24d1",
      "metadata": {
        "id": "d771ba19-0a4e-47e9-be6f-b8b6bd3a24d1"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# %%capture prevents this cell from printing a ton of STDERR stuff to the screen\n",
        "\n",
        "## First, check to see if lightning is installed, if not, install it.\n",
        "##\n",
        "## NOTE: If you **do** need to install something, just know that you may need to\n",
        "##       restart your session for python to find the new module(s).\n",
        "##\n",
        "##       To restart your session:\n",
        "##       - In Google Colab, click on the \"Runtime\" menu and select\n",
        "##         \"Restart Session\" from the pulldown menu\n",
        "##       - In a local jupyter notebook, click on the \"Kernel\" menu and select\n",
        "##         \"Restart Kernel\" from the pulldown menu\n",
        "import pip\n",
        "try:\n",
        "  __import__(\"lightning\")\n",
        "except ImportError:\n",
        "  pip.main(['install', \"lightning\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "13fdd888-e128-40da-ae13-9889b07237a6",
      "metadata": {
        "id": "13fdd888-e128-40da-ae13-9889b07237a6"
      },
      "outputs": [],
      "source": [
        "import torch ## torch let's us create tensors and also provides helper functions\n",
        "import torch.nn as nn ## torch.nn gives us nn.Module(), nn.Embedding() and nn.Linear()\n",
        "import torch.nn.functional as F # This gives us the softmax() and argmax()\n",
        "from torch.optim import Adam ## We will use the Adam optimizer, which is, essentially,\n",
        "                             ## a slightly less stochastic version of stochastic gradient descent.\n",
        "from torch.utils.data import TensorDataset, DataLoader ## We'll store our data in DataLoaders\n",
        "\n",
        "import lightning as L ## Lightning makes it easier to write, optimize and scale our code\n",
        "\n",
        "## NOTE: If you get an error running this block of code, it is probably\n",
        "##       because you installed a new package earlier and forgot to\n",
        "##       restart your session for python to find the new module(s).\n",
        "##\n",
        "##       To restart your session:\n",
        "##       - In Google Colab, click on the \"Runtime\" menu and select\n",
        "##         \"Restart Session\" from the pulldown menu\n",
        "##       - In a local jupyter notebook, click on the \"Kernel\" menu and select\n",
        "##         \"Restart Kernel\" from the pulldown menu"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "311efa7c-fb01-4066-95b5-ae6bc7135d96",
      "metadata": {
        "id": "311efa7c-fb01-4066-95b5-ae6bc7135d96"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60715ee4-2a20-4817-bb6e-976d1ca69d80",
      "metadata": {
        "id": "60715ee4-2a20-4817-bb6e-976d1ca69d80"
      },
      "source": [
        "# Create the datasets that we will use for training Encoder-Decoder model\n",
        "\n",
        "To make the model at least a little bit interesting, we will translate two english phrases, **Let's go** and **to go** into spanish. **Let's go** should translate to **vamos \\<EOS\\>** and **to go** should translate to **ir \\<EOS\\>**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "20226e3b-3177-4df9-a9ca-c1839b44223e",
      "metadata": {
        "id": "20226e3b-3177-4df9-a9ca-c1839b44223e"
      },
      "outputs": [],
      "source": [
        "## first, we create a dictionary that maps vocabulary tokens to id numbers...\n",
        "english_token_to_id = {'lets': 0,\n",
        "                       'to': 1,\n",
        "                       'go': 2,\n",
        "                       '<EOS>': 3 ## <EOS> = end of sequence\n",
        "                      }\n",
        "## ...then we create a dictionary that maps the ids to tokens. This will help us interpret the output.\n",
        "## We use the \"map()\" function to apply the \"reversed()\" function to each tuple (i.e. ('lets', 0)) stored\n",
        "## in the token_to_id dictionary. We then use dict() to make a new dictionary from the\n",
        "## reversed tuples.\n",
        "english_id_to_token = dict(map(reversed, english_token_to_id.items()))\n",
        "\n",
        "spanish_token_to_id = {'ir': 0,\n",
        "                       'vamos': 1,\n",
        "                       'y': 2,\n",
        "                       '<EOS>': 3}\n",
        "spanish_id_to_token = dict(map(reversed, spanish_token_to_id.items()))\n",
        "\n",
        "inputs = torch.tensor([[english_token_to_id[\"lets\"],\n",
        "                        english_token_to_id[\"go\"]],\n",
        "\n",
        "                       [english_token_to_id[\"to\"],\n",
        "                        english_token_to_id[\"go\"]]])\n",
        "\n",
        "labels = torch.tensor([[spanish_token_to_id[\"vamos\"],\n",
        "                        spanish_token_to_id[\"<EOS>\"]],\n",
        "\n",
        "                       [spanish_token_to_id[\"ir\"],\n",
        "                        spanish_token_to_id[\"<EOS>\"]]])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1df92bb-5280-4277-a3dc-87e7ddfe1f3d",
      "metadata": {
        "id": "a1df92bb-5280-4277-a3dc-87e7ddfe1f3d"
      },
      "source": [
        "Now that we have created the data that we want to train the embeddings with, we'll store it in a `DataLoader`. Since our dataset is so small, using a `DataLoader` is a little bit of an overkill, but it it's easy to do, and it will allow us to easily scale up to a much larger vocabulary when the time comes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "68f90604-5ccd-4d84-8758-99bc9ee96b6f",
      "metadata": {
        "id": "68f90604-5ccd-4d84-8758-99bc9ee96b6f"
      },
      "outputs": [],
      "source": [
        "## Now let's package everything up into a DataLoader...\n",
        "dataset = TensorDataset(inputs, labels)\n",
        "dataloader = DataLoader(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "525cdef6-56c9-4a38-a6e5-ba1349d684b9",
      "metadata": {
        "id": "525cdef6-56c9-4a38-a6e5-ba1349d684b9"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9776d85-3686-4e64-89a0-f6c0651ddac4",
      "metadata": {
        "id": "e9776d85-3686-4e64-89a0-f6c0651ddac4"
      },
      "source": [
        "# Build and Train a Seq2Seq/Encoder-Decoder Model with Attention from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c11ba37c-1809-4a90-aaaf-0916058a999f",
      "metadata": {
        "id": "c11ba37c-1809-4a90-aaaf-0916058a999f"
      },
      "outputs": [],
      "source": [
        "class EncoderDecoderAttention(L.LightningModule):\n",
        "\n",
        "    def __init__(self, max_len=2):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.max_decoder_length = max_len\n",
        "\n",
        "        L.seed_everything(seed=42)\n",
        "\n",
        "        #################################\n",
        "        ##\n",
        "        ## ENCODING\n",
        "        ##\n",
        "        #################################\n",
        "        self.encoder_embeddings = nn.Embedding(num_embeddings=4, # num_embeddings = # of tokens in input vocabulary\n",
        "                                       embedding_dim=1)  # embedding_dim = # of embedding values per token\n",
        "\n",
        "        self.encoder_lstm = nn.LSTM(input_size=1, # encoder_size = number of inputs\n",
        "                                    hidden_size=1,# hidden_size = number of outputs\n",
        "                                    num_layers=1) # num_layers = how many lstm's to stack\n",
        "                                                  #              If there are 2 layers, then the short term memory from the\n",
        "                                                  #              first layer is used as input to the second layer\n",
        "\n",
        "        #################################\n",
        "        ##\n",
        "        ## DECODING\n",
        "        ##\n",
        "        #################################\n",
        "        self.decoder_embeddings = nn.Embedding(num_embeddings=4,\n",
        "                                       embedding_dim=1)\n",
        "\n",
        "        self.decoder_lstm = nn.LSTM(input_size=1,\n",
        "                                    hidden_size=1,\n",
        "                                    num_layers=1)\n",
        "\n",
        "        self.decoder_fc = nn.Linear(in_features=2,  # in_features = number of outputs per lstm * 2 (encoder + decoder)\n",
        "                                   out_features=4) # out_features = number of words in the output vocabulary\n",
        "\n",
        "        self.loss = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "    def forward(self, input, output=None):\n",
        "\n",
        "        # outputs = torch.zeros(self.max_decoder_length, 4)\n",
        "\n",
        "        #################################\n",
        "        ##\n",
        "        ## ENCODING\n",
        "        ##\n",
        "        #################################\n",
        "        ## first, use the encoder stage to create an intermediate encoding of the input text\n",
        "        encoder_embeddings = self.encoder_embeddings(input)\n",
        "        encoder_lstm_output, (encoder_lstm_hidden, encoder_lstm_cell) = self.encoder_lstm(encoder_embeddings)\n",
        "\n",
        "        #################################\n",
        "        ##\n",
        "        ## DECODING\n",
        "        ##\n",
        "        #################################\n",
        "        ## We start by initializing the decoder with the <EOS> token...\n",
        "        decoder_token_id = torch.tensor([spanish_token_to_id[\"<EOS>\"]])\n",
        "        decoder_embeddings = self.decoder_embeddings(decoder_token_id)\n",
        "        decoder_lstm_output, (decoder_lstm_hidden, decoder_lstm_cell) = self.decoder_lstm(decoder_embeddings,\n",
        "                                                                                          (encoder_lstm_hidden,\n",
        "                                                                                           encoder_lstm_cell))\n",
        "\n",
        "        ## Calculate attention here (using unnormalized cosine similarity, aka, dot product, to score the alignment)\n",
        "        ## In other words, we're trying to find words in the input that are used in the same context as those proposed\n",
        "        ## by the lstm.\n",
        "        sims = torch.matmul(decoder_lstm_output, encoder_lstm_output.transpose(dim0=0, dim1=1))\n",
        "        ## Apply softmax to determine what percent of each token's value to\n",
        "        ## use in the final attention values.\n",
        "        attention_percents = F.softmax(sims, dim=1)\n",
        "\n",
        "        ## Scale the values by their associated percentages and add them up.\n",
        "        attention_values = torch.matmul(attention_percents, encoder_lstm_output)\n",
        "\n",
        "        ## lastly, we need to concatenate the attention values with the short term\n",
        "        ## memories from the first decoder lstm\n",
        "        values_to_fc_layer = torch.cat((attention_values, decoder_lstm_output), 1)\n",
        "\n",
        "        output_values = self.decoder_fc(values_to_fc_layer)\n",
        "        outputs = output_values\n",
        "        predicted_id = torch.tensor([torch.argmax(output_values)])\n",
        "        predicted_ids = predicted_id\n",
        "\n",
        "        for i in range(1, self.max_decoder_length):\n",
        "            if (output == None): # using the model...\n",
        "                if (predicted_id == spanish_token_to_id[\"<EOS>\"]): # if the prediction is <EOS>, then we are done\n",
        "                    break\n",
        "                decoder_embeddings = self.decoder_embeddings(predicted_id)\n",
        "            else:\n",
        "                decoder_embeddings = self.decoder_embeddings(torch.tensor([output[i-1]]))\n",
        "\n",
        "            decoder_lstm_output, (decoder_lstm_hidden, decoder_lstm_cell) = self.decoder_lstm(decoder_embeddings,\n",
        "                                                                                              (decoder_lstm_hidden,\n",
        "                                                                                               decoder_lstm_cell))\n",
        "            sims = torch.matmul(decoder_lstm_output, encoder_lstm_output.transpose(dim0=0, dim1=1))\n",
        "            ## Apply softmax to determine what percent of each token's value to\n",
        "            ## use in the final attention values.\n",
        "            attention_percents = F.softmax(sims, dim=1)\n",
        "\n",
        "            ## Scale the values by their associated percentages and add them up.\n",
        "            attention_values = torch.matmul(attention_percents, encoder_lstm_output)\n",
        "\n",
        "            values_to_fc_layer = torch.cat((attention_values, decoder_lstm_output), 1)\n",
        "\n",
        "            output_values = self.decoder_fc(values_to_fc_layer)\n",
        "            # outputs[i] = output_values\n",
        "            outputs = torch. cat((outputs, output_values), 0)\n",
        "            predicted_id = torch.tensor([torch.argmax(output_values)])\n",
        "            predicted_ids = torch.cat((predicted_ids, predicted_id))\n",
        "\n",
        "        return(outputs)\n",
        "\n",
        "\n",
        "    def configure_optimizers(self): # this configures the optimizer we want to use for backpropagation.\n",
        "        return Adam(self.parameters(), lr=0.1) ## NOTE: Setting the learning rate to 0.1 trains way faster than\n",
        "                                               ## using the default learning rate, lr=0.001\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx): # take a step during gradient descent.\n",
        "        input_tokens, labels = batch # collect input\n",
        "        output = self.forward(input_tokens[0], labels[0]) # run input through the neural network\n",
        "        loss = self.loss(output, labels[0]) ## self.loss = cross entropy\n",
        "        ###################\n",
        "        ##\n",
        "        ## Logging the loss\n",
        "        ##\n",
        "        ###################\n",
        "        # self.log(\"train_loss\", loss)\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30c22b96-cd77-4104-9e44-42dab6ad31df",
      "metadata": {
        "id": "30c22b96-cd77-4104-9e44-42dab6ad31df"
      },
      "source": [
        "Now that we have created the `seq2seq_attention()` class, let's just run the phrase **Let's go** through it to see what it gets translated into."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "178aaa49-553a-4e41-96ef-b5255f09638f",
      "metadata": {
        "id": "178aaa49-553a-4e41-96ef-b5255f09638f",
        "outputId": "a1a357a3-6c3a-43f1-b0c5-c038fc6333f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: Seed set to 42\n",
            "INFO:lightning.fabric.utilities.seed:Seed set to 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated text:\n",
            "\t y\n",
            "\t y\n"
          ]
        }
      ],
      "source": [
        "model = EncoderDecoderAttention()\n",
        "outputs = model.forward(input=torch.tensor([english_token_to_id[\"lets\"],\n",
        "                                            english_token_to_id[\"go\"]]), ## translate \"lets go\", we should get \"vamos <EOS>\"\n",
        "                        output=None)\n",
        "\n",
        "print(\"Translated text:\")\n",
        "predicted_ids = torch.argmax(outputs, dim=1)\n",
        "for id in predicted_ids:\n",
        "    print(\"\\t\", spanish_id_to_token[id.item()])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9c98aa3-ff1c-4ace-9311-df66a064c2c2",
      "metadata": {
        "id": "b9c98aa3-ff1c-4ace-9311-df66a064c2c2"
      },
      "source": [
        "And we see that **Let's go** was translated to **y y** instead of what we wanted, which was **vamos \\<EOS\\>**. So let's train the model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8c65240b-555d-46c6-9650-193c1bc939c8",
      "metadata": {
        "id": "8c65240b-555d-46c6-9650-193c1bc939c8",
        "outputId": "f514b837-ffa3-49aa-ae7a-d58a7b19824a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 837,
          "referenced_widgets": [
            "2ed9ccea33d34dc8871efe1459dfda9e",
            "b7fc928d3f6d42b9bdcbb56b77cb5f86",
            "8bdb314cc47741a68a814adc473c5fde",
            "cea9929be24b439ca6c93336f85160d6",
            "359f2047f22b4cd990c99bfc4a28c5bf",
            "9a8d6dc8d52a4de1b9e49f7c09df7682",
            "fc5790bcb6f344f2a29ffa0a7b20212f",
            "00705e52596f43aab786fc53d5addb89",
            "e101b31c61c84d0e99bf6f95985c9948",
            "1ad585759ba4402bbfbc7d2fb5fb8207",
            "68865f7428af4f98a631266bb46d5889"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "INFO: GPU available: False, used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO: \n",
            "  | Name               | Type             | Params | Mode \n",
            "----------------------------------------------------------------\n",
            "0 | encoder_embeddings | Embedding        | 4      | train\n",
            "1 | encoder_lstm       | LSTM             | 16     | train\n",
            "2 | decoder_embeddings | Embedding        | 4      | train\n",
            "3 | decoder_lstm       | LSTM             | 16     | train\n",
            "4 | decoder_fc         | Linear           | 12     | train\n",
            "5 | loss               | CrossEntropyLoss | 0      | train\n",
            "----------------------------------------------------------------\n",
            "52        Trainable params\n",
            "0         Non-trainable params\n",
            "52        Total params\n",
            "0.000     Total estimated model params size (MB)\n",
            "6         Modules in train mode\n",
            "0         Modules in eval mode\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name               | Type             | Params | Mode \n",
            "----------------------------------------------------------------\n",
            "0 | encoder_embeddings | Embedding        | 4      | train\n",
            "1 | encoder_lstm       | LSTM             | 16     | train\n",
            "2 | decoder_embeddings | Embedding        | 4      | train\n",
            "3 | decoder_lstm       | LSTM             | 16     | train\n",
            "4 | decoder_fc         | Linear           | 12     | train\n",
            "5 | loss               | CrossEntropyLoss | 0      | train\n",
            "----------------------------------------------------------------\n",
            "52        Trainable params\n",
            "0         Non-trainable params\n",
            "52        Total params\n",
            "0.000     Total estimated model params size (MB)\n",
            "6         Modules in train mode\n",
            "0         Modules in eval mode\n",
            "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2ed9ccea33d34dc8871efe1459dfda9e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: `Trainer.fit` stopped: `max_epochs=20` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=20` reached.\n"
          ]
        }
      ],
      "source": [
        "trainer = L.Trainer(max_epochs=20, accelerator=\"cpu\")\n",
        "trainer.fit(model, train_dataloaders=dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b30a063f-1b32-4681-8232-68c781e84425",
      "metadata": {
        "id": "b30a063f-1b32-4681-8232-68c781e84425"
      },
      "source": [
        "Now let's see if the model correctly translates **Let's go** into **vamos \\<EOS\\>**..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9fe9d8e7-0cac-44bb-9bf0-5473e75080fd",
      "metadata": {
        "id": "9fe9d8e7-0cac-44bb-9bf0-5473e75080fd",
        "outputId": "e7bc8161-742c-433e-c484-4ea112bd0a4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated text:\n",
            "\t vamos\n",
            "\t <EOS>\n"
          ]
        }
      ],
      "source": [
        "outputs = model.forward(input=torch.tensor([english_token_to_id[\"lets\"],\n",
        "                                            english_token_to_id[\"go\"]]), ## translate \"lets go\", we should get \"vamos <EOS>\"\n",
        "                        output=None)\n",
        "\n",
        "print(\"Translated text:\")\n",
        "predicted_ids = torch.argmax(outputs, dim=1)\n",
        "for id in predicted_ids:\n",
        "    print(\"\\t\", spanish_id_to_token[id.item()])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b608d05-bb24-4e0b-983a-851167008089",
      "metadata": {
        "id": "6b608d05-bb24-4e0b-983a-851167008089"
      },
      "source": [
        "...and it does!\n",
        "\n",
        "**BAM!**\n",
        "\n",
        "Now let's see if the model correctly translates **to go** to **ir \\<EOS\\>**..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "480f7187-afbb-4a05-8c5d-e325354083f0",
      "metadata": {
        "id": "480f7187-afbb-4a05-8c5d-e325354083f0",
        "outputId": "d1af9e5a-9871-4400-8e86-86c63ad5083c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated text:\n",
            "\t ir\n",
            "\t <EOS>\n"
          ]
        }
      ],
      "source": [
        "outputs = model.forward(input=torch.tensor([english_token_to_id[\"to\"],\n",
        "                                            english_token_to_id[\"go\"]]), ## translate \"to go\", we should get \"ir <EOS>\"\n",
        "                        output=None)\n",
        "\n",
        "print(\"Translated text:\")\n",
        "predicted_ids = torch.argmax(outputs, dim=1)\n",
        "for id in predicted_ids:\n",
        "    print(\"\\t\", spanish_id_to_token[id.item()])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd8e19ae-442c-440e-a041-d1da42685bce",
      "metadata": {
        "id": "dd8e19ae-442c-440e-a041-d1da42685bce"
      },
      "source": [
        "...and it does!\n",
        "\n",
        "**DOUBLE BAM!!**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fcbf011-b04c-4ef0-a6bf-68cd071bb36d",
      "metadata": {
        "id": "6fcbf011-b04c-4ef0-a6bf-68cd071bb36d"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8708342b-834e-4a71-bb2f-d0e22df2ccac",
      "metadata": {
        "id": "8708342b-834e-4a71-bb2f-d0e22df2ccac"
      },
      "source": [
        "# Print Out Weights and Biases"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3fbf538-9d69-4dc1-b5d4-9bb347bbe6f1",
      "metadata": {
        "id": "a3fbf538-9d69-4dc1-b5d4-9bb347bbe6f1"
      },
      "source": [
        "Relatively early in **Chapter 11** we had a note saying that if you wanted to do the math by hand, you'd have to print out the weights and biases when we code it. So, here are the weights and biases..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "16ba289e-2127-4622-886b-93b1948192de",
      "metadata": {
        "id": "16ba289e-2127-4622-886b-93b1948192de",
        "outputId": "944332cf-53db-4988-d191-be20591bb14c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After optimization, the parameters are...\n",
            "encoder_embeddings.weight tensor([[-1.7787],\n",
            "        [ 2.0923],\n",
            "        [-0.1636],\n",
            "        [ 0.2303]])\n",
            "encoder_lstm.weight_ih_l0 tensor([[-1.6984],\n",
            "        [-2.2462],\n",
            "        [ 1.4013],\n",
            "        [-1.5993]])\n",
            "encoder_lstm.weight_hh_l0 tensor([[-0.7744],\n",
            "        [-1.1266],\n",
            "        [ 1.5920],\n",
            "        [-0.4829]])\n",
            "encoder_lstm.bias_ih_l0 tensor([ 1.8437,  1.4772, -1.0555, -0.0629])\n",
            "encoder_lstm.bias_hh_l0 tensor([ 0.6121,  1.2121, -0.9948,  0.3456])\n",
            "decoder_embeddings.weight tensor([[-0.8066],\n",
            "        [-2.2045],\n",
            "        [-0.9890],\n",
            "        [ 2.8455]])\n",
            "decoder_lstm.weight_ih_l0 tensor([[-0.4647],\n",
            "        [ 2.1268],\n",
            "        [-1.5542],\n",
            "        [ 0.4553]])\n",
            "decoder_lstm.weight_hh_l0 tensor([[-1.3270],\n",
            "        [ 1.8276],\n",
            "        [-0.5032],\n",
            "        [-1.9216]])\n",
            "decoder_lstm.bias_ih_l0 tensor([ 1.8560, -0.7807,  0.7111,  3.0293])\n",
            "decoder_lstm.bias_hh_l0 tensor([ 2.1653, -0.9467,  0.8675,  2.3154])\n",
            "decoder_fc.weight tensor([[ 0.4913, -1.7664],\n",
            "        [-1.1043, -1.1885],\n",
            "        [ 1.4996,  0.7445],\n",
            "        [-1.1738,  3.0864]])\n",
            "decoder_fc.bias tensor([ 0.2659,  0.0945, -1.5209,  1.2383])\n"
          ]
        }
      ],
      "source": [
        "print(\"After optimization, the parameters are...\")\n",
        "for name, param in model.named_parameters():\n",
        "    print(name, param.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a9e7faa-9869-4626-9c17-01c5decdf855",
      "metadata": {
        "id": "6a9e7faa-9869-4626-9c17-01c5decdf855"
      },
      "source": [
        "**TRIPLE BAM!!!**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2ed9ccea33d34dc8871efe1459dfda9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b7fc928d3f6d42b9bdcbb56b77cb5f86",
              "IPY_MODEL_8bdb314cc47741a68a814adc473c5fde",
              "IPY_MODEL_cea9929be24b439ca6c93336f85160d6"
            ],
            "layout": "IPY_MODEL_359f2047f22b4cd990c99bfc4a28c5bf"
          }
        },
        "b7fc928d3f6d42b9bdcbb56b77cb5f86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a8d6dc8d52a4de1b9e49f7c09df7682",
            "placeholder": "​",
            "style": "IPY_MODEL_fc5790bcb6f344f2a29ffa0a7b20212f",
            "value": "Epoch 19: 100%"
          }
        },
        "8bdb314cc47741a68a814adc473c5fde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00705e52596f43aab786fc53d5addb89",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e101b31c61c84d0e99bf6f95985c9948",
            "value": 2
          }
        },
        "cea9929be24b439ca6c93336f85160d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ad585759ba4402bbfbc7d2fb5fb8207",
            "placeholder": "​",
            "style": "IPY_MODEL_68865f7428af4f98a631266bb46d5889",
            "value": " 2/2 [00:00&lt;00:00, 53.60it/s, v_num=0]"
          }
        },
        "359f2047f22b4cd990c99bfc4a28c5bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "9a8d6dc8d52a4de1b9e49f7c09df7682": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc5790bcb6f344f2a29ffa0a7b20212f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00705e52596f43aab786fc53d5addb89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e101b31c61c84d0e99bf6f95985c9948": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1ad585759ba4402bbfbc7d2fb5fb8207": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68865f7428af4f98a631266bb46d5889": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}