{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/StatQuest-Neural-Networks-and-AI/blob/main/chapter_10/chapter_10_seq2seq_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "453a4f64-f611-4b3f-b452-2eee89913606",
      "metadata": {
        "id": "453a4f64-f611-4b3f-b452-2eee89913606"
      },
      "source": [
        "# The StatQuest Illustrated Guide to Neural Networks and AI\n",
        "## Chapter 10 - Seq2Seq and Encoder-Decoder Models with LSTMs\n",
        "\n",
        "Copyright 2024, Joshua Starmer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0164aa71-2955-426d-b7af-8571a26c1f66",
      "metadata": {
        "id": "0164aa71-2955-426d-b7af-8571a26c1f66"
      },
      "source": [
        "This tutorial is from the book, **[The StatQuest Illustrated Guide to Neural Networks and AI](https://www.amazon.com/dp/B0DRS71QVQ)**.\n",
        "\n",
        "In this notebook, we will build and train a Seq2Seq or Encoder-Deocder model with 2 layers of LSTMs, each layer with 2 stacks of LSTMs as seen in the picture below.\n",
        "\n",
        "<img src=\"https://github.com/StatQuest/signa/blob/main/chapter_10/images/full_model.png?raw=1\" alt=\"an encoder-decoder model with 2 layers of LSTMs, each layer with 2 stacks of LSTMs\" style=\"width: 800px;\">\n",
        "\n",
        "In this tutorial, you will...\n",
        "\n",
        "**NOTE:**\n",
        "This tutorial assumes that you have read through the chapter on **Seq2Seq and Encoder-Decoder Models** in **The StatQuest Illustrated Guide to Neural Networks and AI**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "058c8f08-2aba-4947-b197-2ec6add52aaa",
      "metadata": {
        "id": "058c8f08-2aba-4947-b197-2ec6add52aaa"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4aa40bf7-32ee-4278-91ba-b78a222c68ed",
      "metadata": {
        "id": "4aa40bf7-32ee-4278-91ba-b78a222c68ed"
      },
      "source": [
        "# Import the modules that will do all the work"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The very first thing we need to do is load a bunch of Python modules. Python itself is just a basic programming language. These modules give us extra functionality to create and train a Neural Network."
      ],
      "metadata": {
        "id": "gDz-iuf9wBoV"
      },
      "id": "gDz-iuf9wBoV"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6b27bdee-8e09-419d-ba2d-b7c4c32caebb",
      "metadata": {
        "id": "6b27bdee-8e09-419d-ba2d-b7c4c32caebb"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# %%capture prevents this cell from printing a ton of STDERR stuff to the screen\n",
        "\n",
        "## First, check to see if lightning is installed, if not, install it.\n",
        "##\n",
        "## NOTE: If you **do** need to install something, just know that you may need to\n",
        "##       restart your session for python to find the new module(s).\n",
        "##\n",
        "##       To restart your session:\n",
        "##       - In Google Colab, click on the \"Runtime\" menu and select\n",
        "##         \"Restart Session\" from the pulldown menu\n",
        "##       - In a local jupyter notebook, click on the \"Kernel\" menu and select\n",
        "##         \"Restart Kernel\" from the pulldown menu\n",
        "import pip\n",
        "try:\n",
        "  __import__(\"lightning\")\n",
        "except ImportError:\n",
        "  pip.main(['install', \"lightning\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1a5f3628-5b95-4302-b365-56fc93b63d40",
      "metadata": {
        "id": "1a5f3628-5b95-4302-b365-56fc93b63d40"
      },
      "outputs": [],
      "source": [
        "import torch ## torch let's us create tensors and also provides helper functions\n",
        "import torch.nn as nn ## torch.nn gives us nn.Module(), nn.Embedding() and nn.Linear()\n",
        "import torch.nn.functional as F # This gives us the softmax() and argmax()\n",
        "from torch.optim import Adam ## We will use the Adam optimizer, which is, essentially,\n",
        "                             ## a slightly less stochastic version of stochastic gradient descent.\n",
        "from torch.utils.data import TensorDataset, DataLoader ## We'll store our data in DataLoaders\n",
        "\n",
        "import lightning as L ## Lightning makes it easier to write, optimize and scale our code\n",
        "\n",
        "## NOTE: If you get an error running this block of code, it is probably\n",
        "##       because you installed a new package earlier and forgot to\n",
        "##       restart your session for python to find the new module(s).\n",
        "##\n",
        "##       To restart your session:\n",
        "##       - In Google Colab, click on the \"Runtime\" menu and select\n",
        "##         \"Restart Session\" from the pulldown menu\n",
        "##       - In a local jupyter notebook, click on the \"Kernel\" menu and select\n",
        "##         \"Restart Kernel\" from the pulldown menu"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6007fea2-787e-433b-85c8-3ab21d6e117d",
      "metadata": {
        "id": "6007fea2-787e-433b-85c8-3ab21d6e117d"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd3eadcd-d980-4bf1-a713-2304695235a6",
      "metadata": {
        "id": "bd3eadcd-d980-4bf1-a713-2304695235a6"
      },
      "source": [
        "# Create the datasets that we will use for training Encoder-Decoder model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75bf6e48-e2b0-46fe-8df3-43526bc1a43a",
      "metadata": {
        "id": "75bf6e48-e2b0-46fe-8df3-43526bc1a43a"
      },
      "source": [
        "To make the model at least a little bit interesting, we will translate two english phrases, **Let's go** and **to go** into spanish. **Let's go** should translate to **vamos \\<EOS\\>** and **to go** should translate to **ir \\<EOS\\>**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ea923c1e-2527-4bee-bffa-dcf9a2c4c430",
      "metadata": {
        "id": "ea923c1e-2527-4bee-bffa-dcf9a2c4c430"
      },
      "outputs": [],
      "source": [
        "## first, we create a dictionary that maps vocabulary tokens to id numbers...\n",
        "english_token_to_id = {'lets': 0,\n",
        "                       'to': 1,\n",
        "                       'go': 2,\n",
        "                       '<EOS>': 3 ## <EOS> = end of sequence\n",
        "                      }\n",
        "## ...then we create a dictionary that maps the ids to tokens. This will help us interpret the output.\n",
        "## We use the \"map()\" function to apply the \"reversed()\" function to each tuple (i.e. ('lets', 0)) stored\n",
        "## in the token_to_id dictionary. We then use dict() to make a new dictionary from the reversed tuples.\n",
        "english_id_to_token = dict(map(reversed, english_token_to_id.items()))\n",
        "\n",
        "spanish_token_to_id = {'ir': 0,\n",
        "                       'vamos': 1,\n",
        "                       'y': 2,\n",
        "                       '<EOS>': 3}\n",
        "spanish_id_to_token = dict(map(reversed, spanish_token_to_id.items()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "english_id_to_token"
      ],
      "metadata": {
        "id": "HeinB67hv34w",
        "outputId": "c52d92fe-5c39-4b58-9093-968cf14f7a2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "HeinB67hv34w",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'lets', 1: 'to', 2: 'go', 3: '<EOS>'}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spanish_id_to_token"
      ],
      "metadata": {
        "id": "t-PXvrlIv4Wr",
        "outputId": "0daacda0-2ad3-45c3-8e4b-20176020304f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "t-PXvrlIv4Wr",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'ir', 1: 'vamos', 2: 'y', 3: '<EOS>'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor([[english_token_to_id[\"lets\"],\n",
        "                        english_token_to_id[\"go\"]],\n",
        "\n",
        "                       [english_token_to_id[\"to\"],\n",
        "                        english_token_to_id[\"go\"]]])\n",
        "\n",
        "labels = torch.tensor([[spanish_token_to_id[\"vamos\"],\n",
        "                        spanish_token_to_id[\"<EOS>\"]],\n",
        "\n",
        "                       [spanish_token_to_id[\"ir\"],\n",
        "                        spanish_token_to_id[\"<EOS>\"]]])"
      ],
      "metadata": {
        "id": "Z0K5hwCvv1rT"
      },
      "id": "Z0K5hwCvv1rT",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "26db742d-9789-43d9-905a-1d5be57cc58e",
      "metadata": {
        "id": "26db742d-9789-43d9-905a-1d5be57cc58e"
      },
      "source": [
        "Now that we have created the data that we want to train the embeddings with, we'll store it in a `DataLoader`. Since our dataset is so small, using a `DataLoader` is a little bit of an overkill, but it it's easy to do, and it will allow us to easily scale up to a much larger vocabulary when the time comes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d39473af-725d-4ac9-bdc6-8927638daaa2",
      "metadata": {
        "id": "d39473af-725d-4ac9-bdc6-8927638daaa2"
      },
      "outputs": [],
      "source": [
        "## Now let's package everything up into a DataLoader...\n",
        "dataset = TensorDataset(inputs, labels)\n",
        "dataloader = DataLoader(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7808876d-0c42-4adf-b576-43c29167ed28",
      "metadata": {
        "id": "7808876d-0c42-4adf-b576-43c29167ed28"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc4230f3-17e4-4923-b08c-461b092dfcfc",
      "metadata": {
        "id": "fc4230f3-17e4-4923-b08c-461b092dfcfc"
      },
      "source": [
        "# Build and Train a Seq2Seq/Encoder-Decoder Model from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2fff9373-167f-4037-bea1-345183880af1",
      "metadata": {
        "id": "2fff9373-167f-4037-bea1-345183880af1"
      },
      "outputs": [],
      "source": [
        "class EncoderDecoder(L.LightningModule):\n",
        "\n",
        "    def __init__(self, max_len=2):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.max_output_length = max_len\n",
        "\n",
        "        L.seed_everything(seed=420)\n",
        "\n",
        "        #################################\n",
        "        ##\n",
        "        ## ENCODING\n",
        "        ##\n",
        "        #################################\n",
        "        self.encoder_embeddings = nn.Embedding(num_embeddings=4, # num_embeddings = # of words in input vocabulary\n",
        "                                       embedding_dim=2)  # embedding_dim = 2 numbers per embedding\n",
        "\n",
        "        self.encoder_lstm = nn.LSTM(input_size=2, # input_size = number of inputs (2 numbers per word)\n",
        "                                    hidden_size=2,# hidden_size = number of outputs (2 per word per layer)\n",
        "                                    num_layers=2) # num_layers = how many lstm's to stack\n",
        "                                                  #          If there are 2 layers, then the short term memory from the\n",
        "                                                  #          first layer is used as input to the second layer\n",
        "\n",
        "        #################################\n",
        "        ##\n",
        "        ## DECODING\n",
        "        ##\n",
        "        #################################\n",
        "        self.decoder_embeddings = nn.Embedding(num_embeddings=4, embedding_dim=2)\n",
        "\n",
        "        self.decoder_lstm = nn.LSTM(input_size=2,\n",
        "                                    hidden_size=2,\n",
        "                                    num_layers=2)\n",
        "\n",
        "        self.output_layer = nn.Linear(in_features=2,  # in_features = # of outputs per LSTM\n",
        "                                   out_features=4) # out_features = # of words in the output vocabulary\n",
        "\n",
        "        #################################\n",
        "        ##\n",
        "        ## Training\n",
        "        ##\n",
        "        #################################\n",
        "        self.loss = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "    def forward(self, input, output=None):\n",
        "\n",
        "        #################################\n",
        "        ##\n",
        "        ## ENCODING\n",
        "        ##\n",
        "        #################################\n",
        "        ## first, use the encoder stage to create an intermediate encoding of the input text\n",
        "        encoder_embeddings = self.encoder_embeddings(input)\n",
        "        encoder_lstm_output, (encoder_lstm_hidden, encoder_lstm_cell) = self.encoder_lstm(encoder_embeddings)\n",
        "\n",
        "        #################################\n",
        "        ##\n",
        "        ## DECODING\n",
        "        ##\n",
        "        #################################\n",
        "        ## We start by initializing the decoder with the <EOS> token...\n",
        "        decoder_token_id = torch.tensor([spanish_token_to_id[\"<EOS>\"]])\n",
        "        decoder_embeddings = self.decoder_embeddings(decoder_token_id)\n",
        "\n",
        "        decoder_lstm_output, (decoder_lstm_hidden, decoder_lstm_cell) = self.decoder_lstm(decoder_embeddings,\n",
        "                                                                                          (encoder_lstm_hidden,\n",
        "                                                                                           encoder_lstm_cell))\n",
        "\n",
        "        output_values = self.output_layer(decoder_lstm_output)\n",
        "        outputs = output_values\n",
        "\n",
        "        predicted_id = torch.tensor([torch.argmax(output_values)])\n",
        "        predicted_ids = predicted_id\n",
        "\n",
        "        for i in range(1, self.max_output_length):\n",
        "\n",
        "            if (output == None): # using the model...\n",
        "                if (predicted_id == spanish_token_to_id[\"<EOS>\"]): # if the prediction is <EOS>, then we are done\n",
        "                    break\n",
        "                decoder_embeddings = self.decoder_embeddings(predicted_id)\n",
        "            else:\n",
        "                ## run this when training the model\n",
        "                decoder_embeddings = self.decoder_embeddings(torch.tensor([output[i-1]]))\n",
        "\n",
        "            decoder_lstm_output, (decoder_lstm_hidden, decoder_lstm_cell) = self.decoder_lstm(decoder_embeddings,\n",
        "                                                                                              (decoder_lstm_hidden,\n",
        "                                                                                               decoder_lstm_cell))\n",
        "\n",
        "            output_values = self.output_layer(decoder_lstm_output)\n",
        "            outputs = torch.cat((outputs, output_values), 0)\n",
        "            predicted_id = torch.tensor([torch.argmax(output_values)])\n",
        "            predicted_ids = torch.cat((predicted_ids, predicted_id))\n",
        "\n",
        "        return(outputs)\n",
        "\n",
        "\n",
        "    def configure_optimizers(self): # this configures the optimizer we want to use for backpropagation.\n",
        "        return Adam(self.parameters(), lr=0.1) ## NOTE: Setting the learning rate to 0.1 trains way faster than\n",
        "                                               ## using the default learning rate, lr=0.001\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx): # take a step during gradient descent.\n",
        "        input_tokens, labels = batch # collect input\n",
        "        output = self.forward(input_tokens[0], labels[0]) # run input through the neural network\n",
        "        loss = self.loss(output, labels[0]) ## self.loss = cross entropy\n",
        "        ###################\n",
        "        ##\n",
        "        ## Logging the loss\n",
        "        ##\n",
        "        ###################\n",
        "        # self.log(\"train_loss\", loss)\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f16ab35-a57b-42e0-a5df-d49145ff9c36",
      "metadata": {
        "id": "4f16ab35-a57b-42e0-a5df-d49145ff9c36"
      },
      "source": [
        "Now that we have created the `seq2seq()` class, let's just run the phrase **Let's go** through it to see what it gets translated into."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "75fe24ca-9a43-43ce-9f31-3be2de14a6af",
      "metadata": {
        "id": "75fe24ca-9a43-43ce-9f31-3be2de14a6af",
        "outputId": "3a56428f-5964-4c1c-cab8-534ccd9ca0d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: Seed set to 420\n",
            "INFO:lightning.fabric.utilities.seed:Seed set to 420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated text:\n",
            "\t <EOS>\n"
          ]
        }
      ],
      "source": [
        "model = EncoderDecoder()\n",
        "outputs = model.forward(input=torch.tensor([english_token_to_id[\"lets\"],\n",
        "                                            english_token_to_id[\"go\"]]), ## translate \"lets go\", we should get \"vamos <EOS>\"\n",
        "                        output=None)\n",
        "\n",
        "print(\"Translated text:\")\n",
        "predicted_ids = torch.argmax(outputs, dim=1)\n",
        "for id in predicted_ids:\n",
        "    print(\"\\t\", spanish_id_to_token[id.item()])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "414beac8-0122-4194-ab14-0cf80f0b4cf1",
      "metadata": {
        "id": "414beac8-0122-4194-ab14-0cf80f0b4cf1"
      },
      "source": [
        "And we see that **Let's go** was translated to **\\<EOS\\>** instead of what we wanted, which was **vamos \\<EOS\\>**. So let's train the model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "178aaa49-553a-4e41-96ef-b5255f09638f",
      "metadata": {
        "id": "178aaa49-553a-4e41-96ef-b5255f09638f",
        "outputId": "5b4d19ca-85fa-40b5-9d46-be3d344107fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 837,
          "referenced_widgets": [
            "21aa61791a8a452eae65ab52ab016cab",
            "940cf988a9b347a2b364f640c57a5b53",
            "c8a3f99fe65445ed9a14c5e272661dae",
            "561266cf5a10428e9fb4ee80553edbb7",
            "dadac085136d4628b4cc2b9aee3adbca",
            "1fbfe704ce5f44e784e48f8d54014d3a",
            "8b660aad5da34e8c8efbbc5ce1ae734e",
            "dc709eaa94714963a86045d165deeb92",
            "068355d84cd74480912337863dacdefa",
            "c3606ccc663d43d18ea0fe7db1f6cbd2",
            "532c1428855e40ea9853cdc4a43e1c88"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "INFO: GPU available: False, used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO: \n",
            "  | Name               | Type             | Params | Mode \n",
            "----------------------------------------------------------------\n",
            "0 | encoder_embeddings | Embedding        | 8      | train\n",
            "1 | encoder_lstm       | LSTM             | 96     | train\n",
            "2 | decoder_embeddings | Embedding        | 8      | train\n",
            "3 | decoder_lstm       | LSTM             | 96     | train\n",
            "4 | output_layer       | Linear           | 12     | train\n",
            "5 | loss               | CrossEntropyLoss | 0      | train\n",
            "----------------------------------------------------------------\n",
            "220       Trainable params\n",
            "0         Non-trainable params\n",
            "220       Total params\n",
            "0.001     Total estimated model params size (MB)\n",
            "6         Modules in train mode\n",
            "0         Modules in eval mode\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name               | Type             | Params | Mode \n",
            "----------------------------------------------------------------\n",
            "0 | encoder_embeddings | Embedding        | 8      | train\n",
            "1 | encoder_lstm       | LSTM             | 96     | train\n",
            "2 | decoder_embeddings | Embedding        | 8      | train\n",
            "3 | decoder_lstm       | LSTM             | 96     | train\n",
            "4 | output_layer       | Linear           | 12     | train\n",
            "5 | loss               | CrossEntropyLoss | 0      | train\n",
            "----------------------------------------------------------------\n",
            "220       Trainable params\n",
            "0         Non-trainable params\n",
            "220       Total params\n",
            "0.001     Total estimated model params size (MB)\n",
            "6         Modules in train mode\n",
            "0         Modules in eval mode\n",
            "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "21aa61791a8a452eae65ab52ab016cab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: `Trainer.fit` stopped: `max_epochs=40` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=40` reached.\n"
          ]
        }
      ],
      "source": [
        "trainer = L.Trainer(max_epochs=40, accelerator=\"cpu\")\n",
        "trainer.fit(model, train_dataloaders=dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "422747c8-a312-444a-adef-2fd471f8ee3f",
      "metadata": {
        "id": "422747c8-a312-444a-adef-2fd471f8ee3f"
      },
      "source": [
        "Now let's see if the model correctly translates **Let's go** into **vamos \\<EOS\\>**..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "76543606-366c-4fff-9a4d-b7f2cacb8392",
      "metadata": {
        "id": "76543606-366c-4fff-9a4d-b7f2cacb8392",
        "outputId": "07fb22c4-3e8e-4c06-8656-054905704ee3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated text:\n",
            "\t vamos\n",
            "\t <EOS>\n"
          ]
        }
      ],
      "source": [
        "outputs = model.forward(input=torch.tensor([english_token_to_id[\"lets\"],\n",
        "                                            english_token_to_id[\"go\"]]), ## translate \"lets go\", we should get \"vamos <EOS>\"\n",
        "                        output=None)\n",
        "\n",
        "print(\"Translated text:\")\n",
        "predicted_ids = torch.argmax(outputs, dim=1)\n",
        "for id in predicted_ids:\n",
        "    print(\"\\t\", spanish_id_to_token[id.item()])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "366e2428-056b-4cdd-90fd-d873a33fffb9",
      "metadata": {
        "id": "366e2428-056b-4cdd-90fd-d873a33fffb9"
      },
      "source": [
        "...and it does!\n",
        "\n",
        "**BAM!**\n",
        "\n",
        "Now let's see if the model correctly translates **to go** to **ir \\<EOS\\>**..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9b4fa33c-db3b-4973-8871-1cfaf647ac4d",
      "metadata": {
        "id": "9b4fa33c-db3b-4973-8871-1cfaf647ac4d",
        "outputId": "bf85bd6b-00f6-4345-b865-53a0e86eafd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated text:\n",
            "\t ir\n",
            "\t <EOS>\n"
          ]
        }
      ],
      "source": [
        "outputs = model.forward(input=torch.tensor([english_token_to_id[\"to\"],\n",
        "                                            english_token_to_id[\"go\"]]), ## translate \"lets go\", we should get \"vamos <EOS>\"\n",
        "                        output=None)\n",
        "\n",
        "print(\"Translated text:\")\n",
        "predicted_ids = torch.argmax(outputs, dim=1)\n",
        "for id in predicted_ids:\n",
        "    print(\"\\t\", spanish_id_to_token[id.item()])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4338216b-6404-4ac5-8b43-62cb3bce2786",
      "metadata": {
        "id": "4338216b-6404-4ac5-8b43-62cb3bce2786"
      },
      "source": [
        "...and it does!\n",
        "\n",
        "**DOUBLE BAM!!**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0de7a059-86bb-49df-8110-fe296f5f2346",
      "metadata": {
        "id": "0de7a059-86bb-49df-8110-fe296f5f2346"
      },
      "source": [
        "Now that we have model that works, let's just see count the number of parameters we had to train by counting the number of parameters where `requires_grad` was set to `True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6ea1a19d-ff52-40e9-8dd5-2f6b4574c19a",
      "metadata": {
        "id": "6ea1a19d-ff52-40e9-8dd5-2f6b4574c19a",
        "outputId": "bf311bf0-8057-4dc7-a61e-75a2ade0a508",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of trainable parameters: 220\n"
          ]
        }
      ],
      "source": [
        "## count the number of parameters...\n",
        "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"Total number of trainable parameters:\", total_trainable_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11e2d3ce-2070-415a-9d62-769022b2f97a",
      "metadata": {
        "id": "11e2d3ce-2070-415a-9d62-769022b2f97a"
      },
      "source": [
        "So this model has 220 parameters. Compared to modern models used in practical situations, that's not very many. However, we could still spare us the agony of having to train this model whenever we wanted to use it by saving the trained parameters to a file and then loading them when needed. So let's do that."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92cbd775-5471-42d7-9893-d973c41d0cb1",
      "metadata": {
        "id": "92cbd775-5471-42d7-9893-d973c41d0cb1"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a057e20d-f22e-4fb4-9b60-10fa96cbd648",
      "metadata": {
        "id": "a057e20d-f22e-4fb4-9b60-10fa96cbd648"
      },
      "source": [
        "# Saving and loading the trained model weights..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c1f06308-7d9e-4c3f-aaeb-9b27302e5e65",
      "metadata": {
        "id": "c1f06308-7d9e-4c3f-aaeb-9b27302e5e65"
      },
      "outputs": [],
      "source": [
        "## First, save the weights...\n",
        "trainer.save_checkpoint(\"seq2seq_en2es_220_trained.ckpt\") ## NOTE: You can specify a path as part of the filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "509d6b0e-6c4d-4e5d-a7f6-edbe9ad18c3c",
      "metadata": {
        "id": "509d6b0e-6c4d-4e5d-a7f6-edbe9ad18c3c",
        "outputId": "efd843b3-7bfb-455c-8be8-75dd10c9ed04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: Seed set to 420\n",
            "INFO:lightning.fabric.utilities.seed:Seed set to 420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated text:\n",
            "\t vamos\n",
            "\t <EOS>\n"
          ]
        }
      ],
      "source": [
        "## Now let's create a new model and load in the saved weights...\n",
        "new_model = EncoderDecoder.load_from_checkpoint(\"seq2seq_en2es_220_trained.ckpt\")\n",
        "\n",
        "outputs = new_model.forward(input=torch.tensor([english_token_to_id[\"lets\"],\n",
        "                                                english_token_to_id[\"go\"]]),\n",
        "                            output=None)\n",
        "\n",
        "print(\"Translated text:\")\n",
        "predicted_ids = torch.argmax(outputs, dim=1)\n",
        "for id in predicted_ids:\n",
        "    print(\"\\t\", spanish_id_to_token[id.item()])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d85f8334-cd9e-460b-9fa0-3f1cf20865a1",
      "metadata": {
        "id": "d85f8334-cd9e-460b-9fa0-3f1cf20865a1"
      },
      "source": [
        "**TRIPLE BAM!!!**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "21aa61791a8a452eae65ab52ab016cab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_940cf988a9b347a2b364f640c57a5b53",
              "IPY_MODEL_c8a3f99fe65445ed9a14c5e272661dae",
              "IPY_MODEL_561266cf5a10428e9fb4ee80553edbb7"
            ],
            "layout": "IPY_MODEL_dadac085136d4628b4cc2b9aee3adbca"
          }
        },
        "940cf988a9b347a2b364f640c57a5b53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fbfe704ce5f44e784e48f8d54014d3a",
            "placeholder": "​",
            "style": "IPY_MODEL_8b660aad5da34e8c8efbbc5ce1ae734e",
            "value": "Epoch 39: 100%"
          }
        },
        "c8a3f99fe65445ed9a14c5e272661dae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc709eaa94714963a86045d165deeb92",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_068355d84cd74480912337863dacdefa",
            "value": 2
          }
        },
        "561266cf5a10428e9fb4ee80553edbb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3606ccc663d43d18ea0fe7db1f6cbd2",
            "placeholder": "​",
            "style": "IPY_MODEL_532c1428855e40ea9853cdc4a43e1c88",
            "value": " 2/2 [00:00&lt;00:00, 53.42it/s, v_num=0]"
          }
        },
        "dadac085136d4628b4cc2b9aee3adbca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "1fbfe704ce5f44e784e48f8d54014d3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b660aad5da34e8c8efbbc5ce1ae734e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc709eaa94714963a86045d165deeb92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "068355d84cd74480912337863dacdefa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c3606ccc663d43d18ea0fe7db1f6cbd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "532c1428855e40ea9853cdc4a43e1c88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}